[{"figure_path": "2410.15460/charts/charts_4_0.png", "caption": "Figure 1: Visualization of Oscillatory Behavior Across Varying LLM Sizes. Hallucination metrics are evaluated at equidistant checkpoints of the Pythia models, with sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, 12B. Part (a) presents the performance of the Pythia models under the SelfCheckGPT metric. Average performance is indicated by solid lines, while the shaded regions represent the standard deviation. Higher SelfCheckGPT score indicates a higher probability of self-contradiction and higher probability of confabulation. Part (b) depicts the same experimental setup, but hallucination measured on the XSum v2 dataset, where Rouge1 is used as the performance metric. A higher Rouge1 score suggests a better alignment of the generated text to that of the reference summary. For all model sizes, we observe a pronounced trend of high variance and oscillatory behavior in hallucination rates. This fluctuation highlights the models' uncertainty at specific time stamps and emphasizes the need for a robust mitigation strategy to stabilize performance during training.", "description": "Figure 1 visualizes the oscillatory behavior of hallucination metrics across various sizes of LLMs during training, highlighting high variance and uncertainty in model performance.", "section": "2 OSCILLATORY BEHAVIOUR VALIDATION"}, {"figure_path": "2410.15460/charts/charts_6_0.png", "caption": "Figure 2: Comparison of sensitive neuron dropout on inference of Eleuther AI's Pythia various model sizes with random neuron dropout. (a) Average sensitive neuron dropout with standard deviation plotted as scale of the model increases. (b) Average sensitive neuron dropout for hallucinatory inputs and non-hallucinatory inputs. Input size for each test is 80 I.I.D. texts. Sensitive neuron dropping presents a clear, significant reduction in EigenScore compared to that of random neuron dropping across model sizes. Hallucinatory generations experience a larger drop in EigenScore, meaning that our protocol scales with likelihood of hallucination.", "description": "The chart compares the effect of sensitive neuron dropout versus random neuron dropout on EigenScore, showing significant reduction in hallucination likelihood with sensitive neuron dropout, especially in hallucinatory outputs.", "section": "3.2 SENSITIVE NEURON IMPACT ON EIGENSCORES"}, {"figure_path": "2410.15460/charts/charts_8_0.png", "caption": "Figure 3: Efficient EigenScore approximation scaling investigation. The figure shows the difference in computation time between regular EigenScore calculation and EES with a moments value of 20. The x-axis represents the product of the matrix's rows and columns, and the y-axis shows the computation time. As matrix size increases, EES consistently reduces computation time, making it a practical choice for large LLMs.", "description": "Figure 3 compares the computation time of EigenScore and its approximation, EES, across various matrix sizes, demonstrating EES's significant efficiency gains for large LLMs.", "section": "3.3 EFFICIENT EIGENSCORE APPROXIMATION"}, {"figure_path": "2410.15460/charts/charts_10_0.png", "caption": "Figure 4: Regular finetuning vs. SeND on two datasets. (a) presents the results of training Pythia 1B on HELM with regular finetuning and SeND. (b) uses the same training setup as (a), but the LLM is trained on MedHALT 2k. In both plots, performance is reported as the average EES over 5 runs on the validation set. Models are trained until loss convergence. Training with SeND shows a more controlled reduction in EES compared to regular finetuning, suggesting that SeND optimizes for hallucinations as well as loss, with less overall confidence variability during training. Since finetuning without dropout consistently achieves better factual consistency than random dropout in our experiments, we compare SeND directly with standard finetuning.", "description": "Figure 4 shows that SeND training leads to a more controlled reduction in EES compared to regular finetuning on both HELM and MedHALT datasets, indicating that it optimizes for both loss and hallucination reduction.", "section": "4.2 PERFORMANCE OF SEND ON PYTHIA 1B"}, {"figure_path": "2410.15460/charts/charts_15_0.png", "caption": "Figure 5: Net change of sentence embeddings between checkpoints 125,000 and 143,000. Each different colour is a different input text. As depicted, there are specific neurons that go through drastic changes between the two checkpoints of the training regardless of the input.", "description": "The chart visualizes the variability in neuron activations between two training checkpoints, highlighting the existence of sensitive neurons that exhibit drastic changes regardless of the input text.", "section": "3 INTERNAL TRAINING DYNAMICS"}, {"figure_path": "2410.15460/charts/charts_18_0.png", "caption": "Figure 6: Effect of changing number of moments on EES calculation time (seconds). More moments gives more accurate approximation but higher computation time.", "description": "The chart displays the computation time of Efficient EigenScore (EES) with varying numbers of rows in the matrix and different moment values.", "section": "3.3 EFFICIENT EIGENSCORE APPROXIMATION"}, {"figure_path": "2410.15460/charts/charts_18_1.png", "caption": "Figure 7: Performance of SeND on Pythia 1B wih HELM dataset computed with both EES and regular EigenScore. EES is able to closely track the true EigenScore performance metric, showing that it is a good approximator.", "description": "The chart compares the performance of the EigenScore and its approximation, Efficient EigenScore (EES), during the training process of Pythia 1B model on the HELM dataset, showing a strong correlation between the two metrics.", "section": "3.3 EFFICIENT EIGENSCORE APPROXIMATION"}]