<table id='1' style='font-size:18px'><tr><td>Algorithm 1 DataGen: Generate Synthetically Labeled Ranked Preference Dataset (Syn-Pic)</td></tr><tr><td>Input: N prompts (P = {ci}N=1), k T2I Models ({0i}(=1), n Reward Models ({Rv}"=1) Output: Ranked Preference Dataset D Initialize: Synthetic dataset D = ⌀ for cin P do Generate k images x1 x2 , · · . , xk = 01(c), 02(c), . . · , 0k(c) , Initialize preference counts Ci = 0; VA E {1,. . . , k} for each reward model Ri⌀ do Compute scores Ri = Ri⌀ (xi , c); Vi E {1,. . , k} for each pair (i, j) with i ≠ j do if Ri > Rij then Increment preference count Ci = Ci +1 Vi E {1, . · · , k} Compute probabilities ⌀(xi) = n.(ki-1) ; Store entry (c,x1, x2 , · . . , xk, ⌀(x1), ⌀(x2) , . . . , ⌀(xk ( ( ( ( ( ) in D return Ranked Preference Dataset D</td></tr><tr><td>Algorithm 2 RankDPO: Ranking-based Preference Optimization using Syn-Pic</td></tr><tr><td>Input: Ranked Preference Dataset D, Initial model ⌀init, Reference model Oref Input: Pre-defined signal-noise schedule {at, ot}�t=1 Hyper-parameters: # Optimization Steps (m), Learning Rate (7), Divergence Control B Initialize: 0 = ⌀init Output: Fine-tuned model ARankDPO for iter = 0 to m do Sample entry (c, x1 x2 , · · · , xk, ⌀(x1 ) , ⌀(x2), · , ⌀(xk ( ( ( ( ( ) ~ D , Sample timestep t ~ U(1, T), and noise E⌀ ~ N(0, I) Compute noisy image x2 = atxi + �t�i Compute model scores Si 스 s(xi , c,t, 0) = ||e⌀ - e⌀(xt, c)112 - ||�i - Eref(Xt, c)113 Determine ranking T by sorting images based on ⌀(x2) in descending order for each pair (i, j) with i > j in T do Compute pairwise gains: Gij = 2⌀(xi) - 2⌀(xi ) Compute discount factors: D(T(i)) = log(1 + �(i)) and D(T(j)) = log(1 + �(j)) Compute pairwise DCG weights: △ij = |Gij| · D(T(i)) - D(T(j)) Compute pairwise loss: Lij = △inj log o (�� (s(xi, c,t,0) - s(x) c,t,01)) Sum pairwise losses: LRankDPO = - Ei>j Lij Compute gradients graditer = V�LRankDPO Update model parameters: 0 = 0 - 7 · graditer Final ARankDPO = 0 return Fine-tuned model ARankDPO</td></tr><tr><td>Algorithm 3 Generate Syn-Pic and Train RankDPO</td></tr><tr><td>Input: N prompts (P = {ci}N1), k T2I Models ({0i}i=1), n Reward Models ({Rv}:=1) Input: Initial model ⌀init, Reference model ⌀ref, Pre-defined signal-noise schedule {at, ot}}t=1 Hyper-parameters: # Optimization Steps (m), Learning Rate (7), Divergence Control B Output: Fine-tuned model ARankDPO // Generate Synthetically Labeled Ranked Preference dataset D using Algorithm 1 D = DataGen(P, {⌀i}k=1, {Ri⌀}n=1) // Train 0 using Algorithm 2 ARankDPO = RankDPO(D, ⌀init, ⌀ref, {⌀t, ot}t=1,m,7,B)</td></tr></table>