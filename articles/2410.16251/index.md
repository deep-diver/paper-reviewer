---
title: "Can Knowledge Editing Really Correct Hallucinations?"
summary: "HalluEditBench: A new benchmark reveals knowledge editing's limitations in truly fixing LLM hallucinations, offering valuable insights for future improvements."
categories: ["AI Generated"]
tags: ["ðŸ”– 24-10-21", "ðŸ¤— 24-10-25"]
showSummary: true
date: 2024-10-21
draft: false
---

### TL;DR


{{< lead >}}

This paper introduces HalluEditBench, a new benchmark for evaluating knowledge editing techniques in Large Language Models (LLMs).  It addresses the issue that existing datasets don't ensure LLMs generate hallucinations before editing, making it hard to assess the true effectiveness of knowledge editing.  HalluEditBench creates a large dataset of real-world LLM hallucinations and evaluates editing methods across five dimensions: Efficacy (how well it fixes hallucinations), Generalization (how well the fix applies to different questions), Portability (how well the fix works in related questions), Locality (whether the fix affects unrelated knowledge), and Robustness (how resistant the fix is to manipulation). The study reveals that the effectiveness of knowledge editing is highly dependent on both the LLM and the domain, with parameter-preserving methods showing more robustness but lower generalization. This benchmark provides valuable insights into knowledge editing's potentials and limitations, guiding future research in improving these methods.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.16251" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}

#### Why does it matter?
To summarize the research paper on knowledge editing for correcting hallucinations in LLMs, providing a catchy summary, TL;DR, key takeaways, and its importance to researchers.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Knowledge editing methods' effectiveness varies significantly depending on the LLM and the specific domain. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Existing datasets may overestimate the performance of knowledge editing in correcting real-world hallucinations. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} HalluEditBench offers a more holistic evaluation framework for knowledge editing techniques, considering various aspects like efficacy, generalization, portability, locality, and robustness. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](figures/figures_2_0.png "ðŸ”¼ Figure 1: Framework of HalluEditBench. For real-world hallucinations, we holistically assess the performance of knowledge editing on Efficacy, Generalization, Portability, Locality, and Robustness.")

> The figure illustrates the framework of HalluEditBench, which holistically evaluates knowledge editing methods by assessing their performance across five dimensions: Efficacy, Generalization, Portability, Locality, and Robustness, using real-world hallucinations.





![](charts/charts_5_0.png "ðŸ”¼ Figure 3: Efficacy Scores of Knowledge Editing Methods. The 'overall' refers to the Efficacy Score (%) on the whole HalluEditBench embracing 9 domains for different methods. The Efficacy Score on each domain is also reported. Efficacy scores (%) are measured by the accuracy on Efficacy Evaluation Question-answer Pairs, where the pre-edit scores of each LLM are ensured 0.")

> The chart displays the efficacy scores of different knowledge editing methods across nine domains and three large language models (LLMs).





{{< table-caption caption="ðŸ”½ Table 1: Performance measured by Accuracy (%) of Llama2-7B before editing (â€œPre-editâ€) and after applying typical knowledge editing methods (â€œPost-editâ€) on common existing evaluation datasets." >}}
<br><table id='10' style='font-size:14px'><tr><td>Method</td><td>WikiDatarecent</td><td>ZsRE</td><td>WikiBio</td></tr><tr><td>Pre-edit</td><td>47.40</td><td>37.49</td><td>61.35</td></tr><tr><td>Post-edit (ROME)</td><td>97.37</td><td>96.86</td><td>95.91</td></tr><tr><td>Post-edit (MEMIT)</td><td>97.10</td><td>95.86</td><td>94.68</td></tr><tr><td>Post-edit (FT-L)</td><td>56.30</td><td>53.82</td><td>66.70</td></tr><tr><td>Post-edit (FT-M)</td><td>100.00</td><td>99.98</td><td>100.00</td></tr><tr><td>Post-edit (LoRA)</td><td>100.00</td><td>100.00</td><td>100.00</td></tr></table>{{< /table-caption >}}

> The table shows the accuracy of Llama2-7B before and after applying different knowledge editing methods on existing datasets.



### More visual insights



<details>
<summary>More on charts
</summary>


![](charts/charts_6_0.png "ðŸ”¼ Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions ('rephrase'), Yes-or-No Questions with Yes or No as answers ('yes' or 'no'), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.")

> The chart displays the Generalization Scores of different knowledge editing methods across five question types for three LLMs.


![](charts/charts_7_0.png "ðŸ”¼ Figure 13: Portability Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions when with N hops. The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The domains include â€œbusinessâ€, â€œentertainment")

> The chart displays the Portability scores of different knowledge editing methods across three LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and three domains (business, entertainment, event) with varying hop distances.


![](charts/charts_8_0.png "ðŸ”¼ Figure 3: Efficacy Scores of Knowledge Editing Methods. The 'overall' refers to the Efficacy Score (%) on the whole HalluEditBench embracing 9 domains for different methods. The Efficacy Score on each domain is also reported. Efficacy scores (%) are measured by the accuracy on Efficacy Evaluation Question-answer Pairs, where the pre-edit scores of each LLM are ensured 0.")

> The chart displays the efficacy scores of various knowledge editing methods across different domains and LLMs in correcting real-world hallucinations.


![](charts/charts_9_0.png "ðŸ”¼ Figure 17: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domains include 'geography', 'health', and 'technology'.")

> The chart displays the robustness scores of seven knowledge editing methods across three large language models (LLMs) and three domains, showing the accuracy of the methods against distractions in prompts over ten turns.


![](charts/charts_22_0.png "ðŸ”¼ Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions ('rephrase'), Yes-or-No Questions with Yes or No as answers ('yes' or 'no'), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.")

> The chart displays the Generalization scores of different knowledge editing methods across various question types for three different LLMs.


![](charts/charts_23_0.png "ðŸ”¼ Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions ('rephrase'), Yes-or-No Questions with Yes or No as answers ('yes' or 'no'), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.")

> The chart displays the Generalization Scores of different knowledge editing methods across five question types for three LLMs on the HalluEditBench dataset.


![](charts/charts_23_1.png "ðŸ”¼ Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions ('rephrase'), Yes-or-No Questions with Yes or No as answers ('yes' or 'no'), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.")

> The chart displays the Generalization Scores of various knowledge editing methods across five question types for three different LLMs.


![](charts/charts_23_2.png "ðŸ”¼ Figure 10: Generalization Scores of Knowledge Editing Methods on 3 LLMs and 2 Domains. Generalization Scores (%) are measured by the accuracy on five types of Generalization Evaluation Question-answer Pairs including Rephrased Questions (â€œrephraseâ€), two types of Yes-or-No Questions with Yes or No as answers (â€œyesâ€ or â€œnoâ€), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to the averaged scores over five types of questions. The domains include â€œentertainmentâ€ and â€œeventâ€.")

> The chart displays the Generalization Scores of different knowledge editing methods across three LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) for two domains (entertainment and event), showing the accuracy of each method on various question types.


![](charts/charts_23_3.png "ðŸ”¼ Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions ('rephrase'), Yes-or-No Questions with Yes or No as answers ('yes' or 'no'), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.")

> The chart displays the generalization scores of various knowledge editing methods across five different question types, showing their ability to generalize to different question phrasings.


![](charts/charts_23_4.png "ðŸ”¼ Figure 10: Generalization Scores of Knowledge Editing Methods on 3 LLMs and 2 Domains. Generalization Scores (%) are measured by the accuracy on five types of Generalization Evaluation Question-answer Pairs including Rephrased Questions (â€œrephraseâ€), two types of Yes-or-No Questions with Yes or No as answers (â€œyesâ€ or â€œnoâ€), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to the averaged scores over five types of questions. The domains include â€œentertainmentâ€ and â€œeventâ€.")

> The chart displays the Generalization Scores of different knowledge editing methods across three LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and two domains (entertainment, event).


![](charts/charts_24_0.png "ðŸ”¼ Figure 10: Generalization Scores of Knowledge Editing Methods on 3 LLMs and 2 Domains. Generalization Scores (%) are measured by the accuracy on five types of Generalization Evaluation Question-answer Pairs including Rephrased Questions (â€œrephraseâ€), two types of Yes-or-No Questions with Yes or No as answers (â€œyesâ€ or â€œnoâ€), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to the averaged scores over five types of questions. The domains include â€œentertainmentâ€ and â€œeventâ€.")

> The chart displays the Generalization scores for different knowledge editing methods across three LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and two domains (entertainment, event) using five different question types.


![](charts/charts_25_0.png "ðŸ”¼ Figure 12: Generalization Scores of Knowledge Editing Methods on 3 LLMs and 2 Domains. Generalization Scores (%) are measured by the accuracy on five types of Generalization Evaluation Question-answer Pairs including Rephrased Questions (â€œrephraseâ€), two types of Yes-or-No Questions with Yes or No as answers (â€œyesâ€ or â€œnoâ€), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to the averaged scores over five types of questions. The domain is â€œtechnologyâ€.")

> The chart displays the Generalization Scores of different knowledge editing methods across three LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and two domains (geography and health) based on five types of evaluation questions.


![](charts/charts_26_0.png "ðŸ”¼ Figure 12: Generalization Scores of Knowledge Editing Methods on 3 LLMs and 2 Domains. Generalization Scores (%) are measured by the accuracy on five types of Generalization Evaluation Question-answer Pairs including Rephrased Questions (â€œrephraseâ€), two types of Yes-or-No Questions with Yes or No as answers (â€œyesâ€ or â€œnoâ€), Multi-Choice Questions (â€œmcâ€), Reversed Questions (â€œreversedâ€). The â€œaverageâ€ refers to the averaged scores over five types of questions. The domain is â€œtechnologyâ€.")

> The chart displays the Generalization Scores of different knowledge editing methods across three LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) for the 'technology' domain, broken down by five question types.


![](charts/charts_27_0.png "ðŸ”¼ Figure 13: Portability Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions when with N hops. The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The domains include â€œbusinessâ€, â€œentertainment")

> The chart displays the portability scores of various knowledge editing methods across three large language models (LLMs) and three domains, showing the accuracy of the methods on multi-hop questions.


![](charts/charts_28_0.png "ðŸ”¼ Figure 13: Portability Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions when with N hops. The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The domains include â€œbusinessâ€, â€œentertainmentâ€, and â€œeventâ€.")

> The chart displays the portability scores of different knowledge editing methods across three LLMs and three domains, illustrating their ability to reason across multiple hops of knowledge.


![](charts/charts_29_0.png "ðŸ”¼ Figure 15: Portability Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions when with N hops. The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The domain is â€œartâ€.")

> The chart displays the portability scores of different knowledge editing methods across various hop distances for Llama2-7B, Llama3-8B, and Mistral-v0.3-7B on the art domain.


![](charts/charts_29_1.png "ðŸ”¼ Figure 5: Portability Scores of Knowledge Editing Methods. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions with N hops (N = 1 ~ 6). The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The results for more domains are given in Appendix D.2. The â€œoverallâ€ refers to the Portability Score (%) on the whole HalluEditBench embracing 9 domains.")

> The chart displays the portability scores of various knowledge editing methods across different hop distances (1-6) for Llama3-8B on the 'art' domain, illustrating the ability of these methods to reason about edited knowledge in downstream tasks.


![](charts/charts_29_2.png "ðŸ”¼ Figure 15: Portability Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions when with N hops. The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The domain is â€œartâ€.")

> The chart displays the portability scores of various knowledge editing methods across different hop distances (1-6) for the Mistral-v0.3-7B model on the â€˜artâ€™ domain.


![](charts/charts_30_0.png "ðŸ”¼ Figure 17: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domains include â€œgeographyâ€, â€œhealthâ€, and â€œtechnologyâ€.")

> The chart displays the robustness scores of seven knowledge editing methods across three large language models (LLMs) and three domains, showing the persistence of edited knowledge under various levels of distraction.


![](charts/charts_31_0.png "ðŸ”¼ Figure 3: Efficacy Scores of Knowledge Editing Methods. The 'overall' refers to the Efficacy Score (%) on the whole HalluEditBench embracing 9 domains for different methods. The Efficacy Score on each domain is also reported. Efficacy scores (%) are measured by the accuracy on Efficacy Evaluation Question-answer Pairs, where the pre-edit scores of each LLM are ensured 0.")

> The chart displays the efficacy scores of various knowledge editing methods across different domains and LLMs, showing their effectiveness in correcting hallucinations.


![](charts/charts_32_0.png "ðŸ”¼ Figure 17: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domains include 'geography', 'health', and 'technology'.")

> The chart displays the robustness scores of various knowledge editing methods across three different LLMs and three domains, showing the percentage of times the LLMs maintained the corrected answers even after being prompted with distracting questions.


![](charts/charts_32_1.png "ðŸ”¼ Figure 17: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domains include 'geography', 'health', and 'technology'.")

> The chart displays the robustness scores of different knowledge editing methods across three LLMs and three domains, showing the percentage of 'yes' responses over ten turns of robustness evaluation questions.


![](charts/charts_32_2.png "ðŸ”¼ Figure 17: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domains include 'geography', 'health', and 'technology'.")

> The chart displays the robustness scores of different knowledge editing methods across three LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and three domains (geography, health, technology) over ten turns.


</details>



### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/21.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/22.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/23.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/24.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/25.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/26.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/27.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/28.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/29.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/30.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/31.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/32.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/33.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/34.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/35.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}