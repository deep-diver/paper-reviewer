---
title: "MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models"
summary: "MagicTailor: Precisely personalize images by controlling individual visual components in text-to-image models, overcoming semantic pollution and imbalance for superior results."
categories: ["AI Generated"]
tags: ["ðŸ”– 24-10-17", "ðŸ¤— 24-10-21"]
showSummary: true
date: 2024-10-17
draft: false
---

### TL;DR


{{< lead >}}

Current text-to-image models struggle with precise control over visual concepts. This paper introduces "component-controllable personalization," a new task aiming to modify specific parts (components) of a concept during personalization.  This is challenging due to "semantic pollution" (unwanted elements corrupting the concept) and "semantic imbalance" (uneven learning of concept and component). To tackle this, the researchers propose MagicTailor, a framework with two key techniques: Dynamic Masked Degradation (DM-Deg) dynamically perturbs undesired elements, and Dual-Stream Balancing (DS-Bal) balances learning between the concept and component.  Experiments show MagicTailor significantly outperforms existing methods in this challenging task, generating images with both accurate concept representation and precise component control.  It also shows promise in various applications and collaborations with other generative tools.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.13370" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}

#### Why does it matter?
MagicTailor enhances text-to-image diffusion models by enabling component-controllable personalization, addressing challenges like semantic pollution and imbalance to achieve superior image generation.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} MagicTailor introduces component-controllable personalization for text-to-image diffusion models, allowing fine-grained control over visual concepts. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Dynamic Masked Degradation (DM-Deg) and Dual-Stream Balancing (DS-Bal) effectively address semantic pollution and imbalance, respectively. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} MagicTailor achieves state-of-the-art performance and exhibits potential for various applications, including collaboration with other generative tools. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](figures/figures_1_0.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing example images generated by the proposed MagicTailor framework.





![](charts/charts_8_0.png "ðŸ”¼ Figure 7: Ablation of loss weights. We report CLIP-T for text alignment, and DreamSim for identity fidelity as it is most similar to human judgments (Fu et al., 2023). For reference, we also present the results of the second-best method in Table 1, highlighting our robustness on loss weights.")

> The chart displays the ablation study of loss weights (Î»pres and Î»attn) on two metrics: CLIP-T (text alignment) and DreamSim (identity fidelity), comparing MagicTailor's performance with the second-best method.





{{< table-caption caption="ðŸ”½ Table 1: Quantitative comparisons. We compare our MagicTailor with SOTA methods of personalization based on automatic metrics and user study. The best results are marked in bold." >}}
<table id='2' style='font-size:16px'><tr><td rowspan="2">Methods</td><td colspan="4">Automatic Metrics</td><td colspan="3">User Study</td></tr><tr><td>CLIP-Tâ†‘</td><td>CLIP-I â†‘</td><td>DINO â†‘</td><td>DreamSim â†“</td><td>Text Align. â†‘</td><td>Id. Fidelity â†‘</td><td>Gen. Quality â†‘</td></tr><tr><td>Textual Inversion (Gal et al., 2022)</td><td>0.236</td><td>0.742</td><td>0.620</td><td>0.558</td><td>5.8%</td><td>2.5%</td><td>5.2%</td></tr><tr><td>DreamBooth (Ruiz et al., 2023)</td><td>0.266</td><td>0.841</td><td>0.798</td><td>0.323</td><td>15.3%</td><td>14.7%</td><td>12.5%</td></tr><tr><td>Custom Diffusion (Kumari et al., 2023)</td><td>0.251</td><td>0.797</td><td>0.750</td><td>0.407</td><td>7.1%</td><td>7.7%</td><td>9.8%</td></tr><tr><td>Break-A-Scene (Avrahami et al., 2023)</td><td>0.259</td><td>0.840</td><td>0.780</td><td>0.338</td><td>10.8%</td><td>12.1%</td><td>22.8%</td></tr><tr><td>CLiC (Safaee et al., 2024)</td><td>0.263</td><td>0.764</td><td>0.663</td><td>0.499</td><td>4.5%</td><td>5.1%</td><td>6.2%</td></tr><tr><td>MagicTailor (Ours)</td><td>0.270</td><td>0.854</td><td>0.813</td><td>0.279</td><td>56.5%</td><td>57.9%</td><td>43.4%</td></tr></table>{{< /table-caption >}}

> Table 1 quantitatively compares MagicTailor's performance against state-of-the-art methods in personalization using automatic metrics and a user study.



### More visual insights

<details>
<summary>More on figures
</summary>


![](figures/figures_3_0.png "ðŸ”¼ Figure 2: Major challenges in component-controllable personalization. (a) Semantic pollution: (i) Undesired visual elements may inadvertently disturb the personalized concept. (ii) A simple mask-out strategy is ineffective and causes unintended compositions, whereas (iii) our DM-Deg effectively suppresses unwanted visual semantics, preventing such pollution. (b) Semantic imbalance: (i) Simultaneously learning the concept and component can lead to imbalance, resulting in concept or component distortion (here we present a case for the former). (ii) Our DS-Bal ensures balanced learning, enhancing personalization performance.")

> Figure 2 illustrates the two main challenges in component-controllable personalization: semantic pollution and semantic imbalance, showcasing how the proposed DM-Deg and DS-Bal methods address these issues.


![](figures/figures_3_1.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization and component-controllable personalization tasks, and provides example images generated by the proposed MagicTailor framework.


![](figures/figures_4_0.png "ðŸ”¼ Figure 3: Pipeline overview of MagicTailor. Using reference images as the inputs, MagicTailor fine-tunes a T2I diffusion model to learn both the target concept and component, enabling the generation of images that seamlessly integrate the component into the concept. Two key techniques, Dynamic Masked Degradation (DM-Deg, see Section 3.2) and Dual-Stream Balancing (DS-Bal, see Section 3.3), address the challenges of semantic pollution and semantic imbalance, respectively. For clarity, only one image per concept/component is presented and the warm-up stage is not depicted.")

> The figure illustrates the MagicTailor pipeline, which uses reference images to fine-tune a text-to-image diffusion model, incorporating DM-Deg and DS-Bal to address semantic pollution and imbalance.


![](figures/figures_5_0.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> This figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, along with example images generated by the proposed MagicTailor framework.


![](figures/figures_6_0.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization and component-controllable personalization in text-to-image diffusion models, showing how MagicTailor modifies a visual concept's specific component.


![](figures/figures_6_1.png "ðŸ”¼ Figure 5: Visualization of the learning process. (a) The vanilla learning paradigm lapses into overemphasizing the easier one. (b) DS-Bal effectively balances the learning of the concept and component.")

> Figure 5 visualizes how the Dual-Stream Balancing (DS-Bal) method effectively balances the learning of visual semantics for both concept and component, resolving the semantic imbalance issue.


![](figures/figures_7_0.png "ðŸ”¼ Figure 6: Qualitative comparisons. We present images generated by MagicTailor and the compared methods for various domains. MagicTailor generally achieves promising text alignment, strong identity fidelity, and high generation quality. More results are provided in Appendix D.")

> Figure 6 shows a qualitative comparison of images generated by MagicTailor and other state-of-the-art methods across various domains, highlighting MagicTailor's superior performance in text alignment, identity preservation, and overall image quality.


![](figures/figures_9_0.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates the concept of personalization in text-to-image diffusion models, showcasing how MagicTailor modifies a specific component of a visual concept during the process.


![](figures/figures_9_1.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the task of personalization, component-controllable personalization, and example images generated by the proposed MagicTailor framework.


![](figures/figures_9_2.png "ðŸ”¼ Figure 6: Qualitative comparisons. We present images generated by MagicTailor and the compared methods for various domains. MagicTailor generally achieves promising text alignment, strong identity fidelity, and high generation quality. More results are provided in Appendix D.")

> Figure 6 shows a qualitative comparison of images generated by MagicTailor and other state-of-the-art methods across various domains, highlighting MagicTailor's superior text alignment, identity preservation, and image quality.


![](figures/figures_10_0.png "ðŸ”¼ Figure 3: Pipeline overview of MagicTailor. Using reference images as the inputs, MagicTailor fine-tunes a T2I diffusion model to learn both the target concept and component, enabling the generation of images that seamlessly integrate the component into the concept. Two key techniques, Dynamic Masked Degradation (DM-Deg, see Section 3.2) and Dual-Stream Balancing (DS-Bal, see Section 3.3), address the challenges of semantic pollution and semantic imbalance, respectively. For clarity, only one image per concept/component is presented and the warm-up stage is not depicted.")

> The figure illustrates the pipeline of MagicTailor, a framework that adapts T2I diffusion models for component-controllable personalization, highlighting its key techniques: Dynamic Masked Degradation (DM-Deg) and Dual-Stream Balancing (DS-Bal).


![](figures/figures_10_2.png "ðŸ”¼ Figure 9: Enhancing other generative tools. MagicTailor can conveniently collaborate with a variety of generative tools that focus on other tasks, equipping them with an additional ability to control the concept's component in their pipelines.")

> The figure shows how MagicTailor can be integrated with other generative tools like ControlNet, CSGO, and InstantMesh to enhance their capabilities by adding component-controllable personalization.


![](figures/figures_10_3.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates the concept of personalization and component-controllable personalization, showing how text-to-image diffusion models can learn and reproduce visual concepts, modify specific components, and generate example images using the proposed MagicTailor framework.


![](figures/figures_10_6.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates the personalization and component-controllable personalization tasks, and shows example images generated by the proposed MagicTailor framework.


![](figures/figures_17_0.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization and component-controllable personalization, and provides example images generated by MagicTailor, highlighting the target concept and component.


![](figures/figures_17_1.png "ðŸ”¼ Figure 8: (a) Decoupled generation. MagicTailor can also separately generate the target concept and component, enriching prospective combinations. (b) Controlling multiple components. MagicTailor shows the potential to handle more than one component, highlighting its effectiveness.")

> Figure 8 demonstrates MagicTailor's ability to generate concepts and components separately and to control multiple components simultaneously.


![](figures/figures_17_2.png "ðŸ”¼ Figure 4: Motivation of dynamic intensity. (a) Fixed intensity (ad = 0.5 here) could cause noisy generated images. (b) Our dynamic intensity helps to mitigate noise memorization.")

> The figure illustrates the benefit of using dynamic intensity in the DM-Deg process to mitigate noise memorization during image generation.


![](figures/figures_17_3.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing how MagicTailor modifies a specific component of a visual concept during personalization.


![](figures/figures_17_4.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization, component-controllable personalization, and example images generated by MagicTailor to showcase its effectiveness in adapting text-to-image diffusion models for component-controllable personalization.


![](figures/figures_17_5.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates the concept of personalization in text-to-image diffusion models and introduces a new task, component-controllable personalization, showing examples of images generated by the proposed MagicTailor framework.


![](figures/figures_18_0.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization, component-controllable personalization, and example images generated by the MagicTailor model, highlighting its effectiveness in component-controllable personalization.


![](figures/figures_18_1.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the tasks of personalization and component-controllable personalization in text-to-image diffusion models and shows example images generated by the proposed MagicTailor framework.


![](figures/figures_18_2.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the concepts of personalization and component-controllable personalization in text-to-image diffusion models, and shows example images generated by the proposed MagicTailor framework.


![](figures/figures_18_3.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization, component-controllable personalization, and example images generated by MagicTailor, highlighting its effectiveness in adapting T2I diffusion models for component-controllable personalization.


![](figures/figures_18_4.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization, component-controllable personalization, and example images generated by MagicTailor to demonstrate its effectiveness in adapting T2I diffusion models for component-controllable personalization.


![](figures/figures_18_5.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the concept of personalization in text-to-image diffusion models, showing how to modify a specific component of a visual concept using reference images, and provides example images generated by the proposed MagicTailor framework.


![](figures/figures_19_0.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing example images generated by the proposed MagicTailor framework.


![](figures/figures_19_1.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization, component-controllable personalization, and example images generated by the proposed MagicTailor framework.


![](figures/figures_19_2.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing examples of images generated by the proposed MagicTailor framework.


![](figures/figures_19_3.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates the concept of personalization in text-to-image diffusion models, showing how to modify specific components of a visual concept using reference images and the results generated by the proposed MagicTailor framework.


![](figures/figures_19_4.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing examples of images generated by the proposed MagicTailor framework.


![](figures/figures_19_5.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates the concept of personalization in text-to-image diffusion models, showing how to modify specific components of a visual concept during personalization using the proposed MagicTailor framework.


![](figures/figures_19_6.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization and component-controllable personalization in text-to-image diffusion models, and shows example images generated by the proposed MagicTailor framework.


![](figures/figures_19_7.png "ðŸ”¼ Figure 3: Pipeline overview of MagicTailor. Using reference images as the inputs, MagicTailor fine-tunes a T2I diffusion model to learn both the target concept and component, enabling the generation of images that seamlessly integrate the component into the concept. Two key techniques, Dynamic Masked Degradation (DM-Deg, see Section 3.2) and Dual-Stream Balancing (DS-Bal, see Section 3.3), address the challenges of semantic pollution and semantic imbalance, respectively. For clarity, only one image per concept/component is presented and the warm-up stage is not depicted.")

> The figure illustrates the MagicTailor pipeline, which fine-tunes a text-to-image diffusion model to learn and integrate a target concept and its component using Dynamic Masked Degradation and Dual-Stream Balancing to address semantic pollution and imbalance.


![](figures/figures_19_8.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> Figure 1 illustrates personalization and component-controllable personalization in text-to-image diffusion models, including examples of images generated by MagicTailor.


![](figures/figures_19_9.png "ðŸ”¼ Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.")

> The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing examples of images generated by the proposed MagicTailor framework.


![](figures/figures_20_0.png "ðŸ”¼ Figure 14: More qualitative comparisons. We present images generated by our MagicTailor and SOTA methods of personalization for various domains including characters, animation, buildings, objects, and animals. MagicTailor generally achieves promising text alignment, strong identity fidelity, and high generation quality.")

> Figure 14 presents a qualitative comparison of image generation results from MagicTailor and other state-of-the-art methods across various domains, showcasing MagicTailor's superior performance in terms of text alignment, identity preservation, and overall image quality.


</details>




<details>
<summary>More on tables
</summary>


{{< table-caption caption="ðŸ”½ Table 1: Quantitative comparisons. We compare our MagicTailor with SOTA methods of personalization based on automatic metrics and user study. The best results are marked in bold." >}}
<table id='1' style='font-size:18px'><tr><td colspan="11">Table 2: Ablation of key techniques. Our DM- Table 4: Ablation of DM-Deg. We compare Deg and DS-Bal effectively contribute to a supe- DM-Deg with its variants and the mask-out strat- rior performance trade-off. egy. Our DM-Deg attains superior overall perfor-</td></tr><tr><td>DM-Deg DS-Bal</td><td></td><td>CLIP-Tâ†‘</td><td>CLIP-I â†‘</td><td colspan="2">DINO â†‘ DreamSim â†“</td><td>mance on text alignment and identity fidelity.</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>0.275</td><td>0.837</td><td>0.798</td><td colspan="2">0.317</td><td>Intensity Variants</td><td>CLIP-Tâ†‘</td><td>CLIP-Iâ†‘</td><td>DINO â†‘</td><td>DreamSim â†“</td></tr><tr><td></td><td>0.276</td><td>0.848</td><td>0.809</td><td colspan="2">0.294</td><td>Mask-Out Startegy</td><td>0.270</td><td>0.818</td><td>0.760</td><td>0.375</td></tr><tr><td></td><td>0.270</td><td>0.845</td><td>0.802</td><td colspan="2">0.304</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>V</td><td>0.270</td><td>0.854</td><td>0.813</td><td colspan="2">0.279</td><td>Fixed (a = 0.4)</td><td>0.270 0.271</td><td>0.849</td><td>0.800</td><td>0.297 0.310</td></tr><tr><td colspan="6">Table 3: Ablation of DS-Bal. We compare DS- Bal with its variants, showing its excellence.</td><td>Fixed (a = 0.6)</td><td>0.271</td><td>0.845 0.846</td><td>0.794 0.796</td><td>0.305</td></tr><tr><td colspan="6"></td><td>Fixed (a = 0.8) Linear (Ascent)</td><td>0.270</td><td>0.846</td><td>0.797</td><td>0.307</td></tr><tr><td>U-Net Variants</td><td>CLIP-Tâ†‘</td><td>CLIP-I â†‘</td><td>DINO â†‘</td><td colspan="2">DreamSim â†“</td><td>Linear (Descent)</td><td>0.261</td><td>0.851</td><td>0.802</td><td>0.300</td></tr><tr><td>Fixed (B = 0)</td><td>0.268</td><td>0.850</td><td>0.803</td><td colspan="2">0.293</td><td>Dynamic (Y = 8)</td><td>0.266</td><td>0.850</td><td>0.806</td><td>0.289</td></tr><tr><td>Fixed (B = 1)</td><td>0.270</td><td>0.851</td><td>0.808</td><td colspan="2">0.286</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Momentum (B = 0.5)</td><td>0.268</td><td>0.850</td><td>0.805</td><td colspan="2">0.290</td><td>Dynamic (Y = 16)</td><td>0.268</td><td>0.854</td><td>0.813</td><td>0.282</td></tr><tr><td>Momentum (B = 0.9)</td><td>0.269</td><td>0.850</td><td>0.808</td><td colspan="2">0.288</td><td>Dynamic (Y = 64)</td><td>0.271</td><td>0.852</td><td>0.812</td><td>0.283</td></tr><tr><td>Momentum (Ours)</td><td>0.270</td><td>0.854</td><td>0.813</td><td colspan="2">0.279</td><td>Dynamic (Ours)</td><td>0.270</td><td>0.854</td><td>0.813</td><td>0.279</td></tr></table>{{< /table-caption >}}

> Table 1 quantitatively compares MagicTailor against state-of-the-art methods for personalization using both automatic metrics and a user study.


{{< table-caption caption="ðŸ”½ Table 1: Quantitative comparisons. We compare MagicTailor with SOTA methods of personalization based on automatic metrics and user study. The best results are marked in bold." >}}
<table id='2' style='font-size:14px'><tr><td>Recontextualization</td><td>Restylization</td></tr><tr><td>' <placeholder>, on the beach" ' ' <placeholder>, in the snow" " <placeholder>, at night" <placeholder>, in autumn"</td><td>"<placeholder>, watercolor painting" Â· <placeholder>, Ukiyo-e painting" ' <placeholder>, in Pixel Art style" "<placeholder>, in Von Gogh style" ' ' <placeholder>, in a comic book"</td></tr><tr><td>' <placeholder>, in the jungle" Interaction</td><td>Property Modification</td></tr><tr><td><placeholder>, with clouds in the background" <placeholder>, with flowers in the background"</td><td>"<placeholder>, from 3D rendering" "<placeholder>, in a far view" in a close view"</td></tr><tr><td><placeholder>, near the Eiffel Tower" <placeholder>, on top of water" <placeholder>, in front of the Mount Fuji"</td><td><placeholder>, <placeholder>, made of clay" <placeholder>, made of plastic"</td></tr></table>{{< /table-caption >}}

> Table 1 quantitatively compares MagicTailor's performance against other state-of-the-art personalization methods using automatic metrics and a user study.


{{< table-caption caption="ðŸ”½ Table 2: Ablation of key techniques. Our DM-Deg and DS-Bal effectively contribute to a superior performance trade-off." >}}
<table id='22' style='font-size:18px'><tr><td>Warm-up Variants</td><td>CLIP-Tâ†‘</td><td>CLIP-Iâ†‘</td><td>DINO â†‘</td><td>DreamSim â†“</td></tr><tr><td>w/o Warm-up</td><td>0.272</td><td>0.844</td><td>0.793</td><td>0.320</td></tr><tr><td>w/ Warm-up (Ours)</td><td>0.270</td><td>0.854</td><td>0.813</td><td>0.279</td></tr></table>{{< /table-caption >}}

> Table 2 shows the ablation study of the two key techniques, Dynamic Masked Degradation (DM-Deg) and Dual-Stream Balancing (DS-Bal), demonstrating their effectiveness in improving the performance of the MagicTailor model.


</details>


### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}