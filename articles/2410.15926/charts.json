[{"figure_path": "2410.15926/charts/charts_2_0.png", "caption": "Figure 1: Long-term decay of RoPE [61] in Large Vision Language Models (LVLMs). (a) a schematic view of inference in LVLMs, typically involving a pre-trained vision encoder, a large language model and a projector to map visual tokens to textual space. For each of V visual tokens Svision, we aggregate its information flow to instruction tokens Sinstruct and reshape the aggregation results to 2-D (\u221aV by \u221aV). Applying RoPE on visual tokens introduces long-term decay as illustrated in (c), referring to the phenomenon where information flowing from visual tokens to instruction tokens gradually decays from lower-right region (rightmost visual tokens in the 1-D sequence) to upper-left region (leftmost visual tokens). For instruction tokens, they have much less direct interaction with leftmost visual tokens as compared with rightmost visual tokens, leading to inferior multimodal alignment in the trained LVLMs. (b) and (c) are derived from the adversarial subset of the 3k POPE [41] image-instruction pairs. Best viewed in color.", "description": "The chart visualizes the long-term decay effect of Rotary Position Encoding (RoPE) in Large Vision Language Models (LVLMs), showing how information flow from visual tokens to instruction tokens diminishes with increasing distance when RoPE is applied.", "section": "1 Introduction"}, {"figure_path": "2410.15926/charts/charts_6_0.png", "caption": "Figure 3: An overview for Concentric Causal Attention. Left: Visual Token Re-organization. In comparison to raster-scan positional alignment in (a), we design concentric position alignment in (b) which shortens visual-instruction distance and retains spatial locality for 2-D data like images. Right: Concentric Causal Masking. By default as in (c), a visual token attends to all preceding visual tokens in a 1-D sequence. In contrast, our concentric causal attention in (d) models 2-D continuous positional dependencies among visual tokens, where center visual tokens attend to peripheral ones. Causal masks are V by V where in this case V is 36 for demonstration purpose. Best viewed in color.", "description": "This chart illustrates the proposed Concentric Causal Attention method, comparing raster-scan and concentric positional assignments and their corresponding causal attention masks.", "section": "4 Concentric Causal Attention"}]