[{"figure_path": "2410.18451/tables/table_5_0.html", "caption": "Table 1 | Statistics of the Skywork Reward Preference 80K dataset for reward modeling.", "description": "This table presents the statistics of the Skywork Reward Preference 80K dataset, including the number of pairs, average number of turns, average number of tokens in prompts and responses, completion methods, and annotators.", "section": "3.1. Dataset Mixture"}, {"figure_path": "2410.18451/tables/table_6_0.html", "caption": "Table 1 | Statistics of the Skywork Reward Preference 80K dataset for reward modeling.", "description": "This table presents the statistics of the Skywork Reward Preference 80K dataset, including the number of pairs, average number of tokens in prompts and responses, completion methods and annotators used for each dataset.", "section": "3.1. Dataset Mixture"}, {"figure_path": "2410.18451/tables/table_11_0.html", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "Table 2 presents a performance comparison of various reward models on the RewardBench benchmark, highlighting the superior performance of the Skywork-Reward models.", "section": "4.2. Experimental Results"}, {"figure_path": "2410.18451/tables/table_12_0.html", "caption": "Table 3 | Ablation studies of loss functions that optimize the margin between chosen and rejected responses on Gemma-2-27B.", "description": "Table 3 shows the results of an ablation study comparing different loss functions used to train reward models, focusing on their ability to maximize the margin between chosen and rejected responses, using the Gemma-2-27B model.", "section": "4.3. Potential Prompt Contamination"}, {"figure_path": "2410.18451/tables/table_13_0.html", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "This table compares the performance of different reward models on RewardBench across four categories: Chat, Chat Hard, Safety, and Reasoning.", "section": "4.2. Experimental Results"}, {"figure_path": "2410.18451/tables/table_13_1.html", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "Table 2 presents a performance comparison of various reward models on the RewardBench benchmark, highlighting the superior performance of the Skywork-Reward models trained on the curated 80K dataset.", "section": "4.2. Experimental Results"}]