[{"figure_path": "2410.13782/figures/figures_2_0.png", "caption": "Figure 1: Overall illustration of DPLM-2. (A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates. (B) Multimodal learning and generation of protein structure and sequence with DPLM-2. (C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.", "description": "Figure 1 illustrates the overall architecture of DPLM-2, detailing its structure tokenization, multimodal learning and generation process, and various applications as a protein foundation model.", "section": "3 DPLM-2: A MULTIMODAL DIFFUSION PROTEIN LANGUAGE MODEL"}, {"figure_path": "2410.13782/figures/figures_4_0.png", "caption": "Figure 1: Overall illustration of DPLM-2. (A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates. (B) Multimodal learning and generation of protein structure and sequence with DPLM-2. (C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.", "description": "Figure 1 illustrates the overall architecture of DPLM-2, including structure tokenization, multimodal training and sampling process, and various downstream applications.", "section": "3 DPLM-2: A MULTIMODAL DIFFUSION PROTEIN LANGUAGE MODEL"}, {"figure_path": "2410.13782/figures/figures_7_0.png", "caption": "Figure 1: Overall illustration of DPLM-2. (A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates. (B) Multimodal learning and generation of protein structure and sequence with DPLM-2. (C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.", "description": "Figure 1 provides a comprehensive overview of the DPLM-2 model, illustrating its structure tokenization process, multimodal learning and generation capabilities, and various applications in protein modeling tasks.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13782/figures/figures_7_1.png", "caption": "Figure 1: Overall illustration of DPLM-2. (A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates. (B) Multimodal learning and generation of protein structure and sequence with DPLM-2. (C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.", "description": "Figure 1 illustrates the overall architecture of DPLM-2, including structure tokenization, multimodal training and sampling, and various applications as a protein foundation model.", "section": "3 DPLM-2: A MULTIMODAL DIFFUSION PROTEIN LANGUAGE MODEL"}, {"figure_path": "2410.13782/figures/figures_7_2.png", "caption": "Figure 3: Evaluation of DPLM-2 on unconditional structure-sequence co-generation. Here for designability of co-generated proteins, we use ESMFold to obtain refolded structure of DPLM-2-generated sequence and measure the structural similarity between DPLM-2-generated structure and the refolded structure, which aims to measure the compatibility of the co-generated structure and sequence pairs.", "description": "Figure 3 presents the results of DPLM-2 for unconditional protein generation, showing its ability to generate diverse and high-quality proteins with simultaneous structure-sequence co-generation, as well as its performance in structure novelty, diversity, and length extrapolation.", "section": "4.1 UNCONDITIONAL PROTEIN GENERATION"}]