[{"figure_path": "2410.13787/figures/figures_6_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The figure shows that language models predict their own behavior more accurately than other models, providing evidence for introspection.", "section": "Experiments"}, {"figure_path": "2410.13787/figures/figures_9_0.png", "caption": "Figure 7: Setup to test if models predict their changed behavior. We use the previously self-prediction trained M1 (here, GPT-40) and change its behavior through further finetuning on the object-level behavior of another model (Claude 3.5 Sonnet), creating model Mc. Mc has changed behavior on held-out prompts. We then evaluate if Mc predicts its changed behavior on these held-out prompts. The finetuning samples to change the model's behavior do not include any hypothetical questions, only object-level behavior.", "description": "The figure illustrates an experiment setup to evaluate if a language model can predict its own behavior after its behavior is intentionally changed via finetuning.", "section": "2.1 Experiments Related to Introspection"}]